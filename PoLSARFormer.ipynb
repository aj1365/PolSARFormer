{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc2gCM+97aXepd05i/rZVx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aj1365/PolSARFormer/blob/main/PoLSARFormer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oQ7PxjGnse7"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, Conv3D, Flatten, Dense, Reshape, BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Input\n",
        "from keras.models import Model\n",
        "#from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "\n",
        "from operator import truediv\n",
        "\n",
        "from plotly.offline import init_notebook_mode\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import os\n",
        "import spectral\n",
        "\n",
        "init_notebook_mode(connected=True)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n"
      ],
      "metadata": {
        "id": "wDYV6q4cn00o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "metadata": {
        "id": "u5PkJkEBn2wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData(name):\n",
        "    \n",
        "    data_path = os.path.join(os.getcwd(),'E:/PolSAR/Data/')\n",
        "   \n",
        "    if name == 'Flevoland':\n",
        "        \n",
        "        data = sio.loadmat(os.path.join(data_path, 'Flevoland_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'Flevoland_15cls.mat'))['label']\n",
        "  \n",
        "    if name == 'SanFrancisco':\n",
        "        \n",
        "        data = sio.loadmat(os.path.join(data_path, 'SanFrancisco_T3RF.mat'))['T3RF']\n",
        "        labels = sio.loadmat(os.path.join(data_path, 'SanFrancisco_gt.mat'))['SanFrancisco_gt']\n",
        "\n",
        "    \n",
        "    return data, labels"
      ],
      "metadata": {
        "id": "DHvFr7OLn2zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## GLOBAL VARIABLES\n",
        "dataset = 'SanFrancisco'\n",
        "test_ratio = 0.90\n",
        "windowSize = 12"
      ],
      "metadata": {
        "id": "Xnzf98usn7N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTrainTestSet(X, y, testRatio, randomState=345):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n",
        "                                                        stratify=y)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "_Q80Ui4dn7Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def applyPCA(X, numComponents=75):\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=numComponents, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
        "    return newX, pca"
      ],
      "metadata": {
        "id": "p5eogJ0in7Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padWithZeros(X, margin=2):\n",
        "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
        "    return newX"
      ],
      "metadata": {
        "id": "NAlCJg6wn7WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createImageCubes(X, y, windowSize=8, removeZeroLabels = True):\n",
        "    margin = int((windowSize) / 2)\n",
        "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
        "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin , c - margin:c + margin ]   \n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
        "        patchesLabels = patchesLabels[patchesLabels>0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels"
      ],
      "metadata": {
        "id": "9Xa94aJfn7ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X , Y = loadData(dataset)\n",
        "\n",
        "X=(X-np.min(X))/(np.max(X)-np.min(X))"
      ],
      "metadata": {
        "id": "gGQBfmkJn7d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y.shape, np.min(Y), np.max(Y)"
      ],
      "metadata": {
        "id": "OefkkfUTn22f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1, Y1 = createImageCubes(X, Y, windowSize=windowSize)\n",
        "X1.shape, Y1.shape"
      ],
      "metadata": {
        "id": "obkkBJNVoGQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = X1.reshape((X1.shape[0],windowSize,windowSize,12,1))\n",
        "X1.shape"
      ],
      "metadata": {
        "id": "Fo0YSsk1oGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X1, Y1, test_ratio)\n",
        "\n",
        "np.min(ytrain), np.max(ytrain)"
      ],
      "metadata": {
        "id": "a96HgxC6oGXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras_cv_attention_models.attention_layers import (\n",
        "    ChannelAffine,\n",
        "    CompatibleExtractPatches,\n",
        "    conv2d_no_bias,\n",
        "    drop_block,\n",
        "    layer_norm,\n",
        "    mlp_block,\n",
        "    output_block,\n",
        "    add_pre_post_process,\n",
        ")\n",
        "from keras_cv_attention_models.download_and_load import reload_model_weights\n"
      ],
      "metadata": {
        "id": "8dki9du5oGZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadRelativePositionalKernelBias(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_height=-1, is_heads_first=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_height, self.is_heads_first = input_height, is_heads_first\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input (is_heads_first=False): `[batch, height * width, num_heads, ..., size * size]`\n",
        "        # input (is_heads_first=True): `[batch, num_heads, height * width, ..., size * size]`\n",
        "        blocks, num_heads = (input_shape[2], input_shape[1]) if self.is_heads_first else (input_shape[1], input_shape[2])\n",
        "        size = int(tf.math.sqrt(float(input_shape[-1])))\n",
        "        height = self.input_height if self.input_height > 0 else int(tf.math.sqrt(float(blocks)))\n",
        "        width = blocks // height\n",
        "        pos_size = 2 * size - 1\n",
        "        initializer = tf.initializers.truncated_normal(stddev=0.02)\n",
        "        self.pos_bias = self.add_weight(name=\"positional_embedding\", shape=(num_heads, pos_size * pos_size), initializer=initializer, trainable=True)\n",
        "\n",
        "        idx_hh, idx_ww = tf.range(0, size), tf.range(0, size)\n",
        "        coords = tf.reshape(tf.expand_dims(idx_hh, -1) * pos_size + idx_ww, [-1])\n",
        "        bias_hh = tf.concat([idx_hh[: size // 2], tf.repeat(idx_hh[size // 2], height - size + 1), idx_hh[size // 2 + 1 :]], axis=-1)\n",
        "        bias_ww = tf.concat([idx_ww[: size // 2], tf.repeat(idx_ww[size // 2], width - size + 1), idx_ww[size // 2 + 1 :]], axis=-1)\n",
        "        bias_hw = tf.expand_dims(bias_hh, -1) * pos_size + bias_ww\n",
        "        bias_coords = tf.expand_dims(bias_hw, -1) + coords\n",
        "        bias_coords = tf.reshape(bias_coords, [-1, size**2])[::-1]  # torch.flip(bias_coords, [0])\n",
        "\n",
        "        bias_coords_shape = [bias_coords.shape[0]] + [1] * (len(input_shape) - 4) + [bias_coords.shape[1]]\n",
        "        self.bias_coords = tf.reshape(bias_coords, bias_coords_shape)  # [height * width, 1 * n, size * size]\n",
        "        if not self.is_heads_first:\n",
        "            self.transpose_perm = [1, 0] + list(range(2, len(input_shape) - 1))  # transpose [num_heads, height * width] -> [height * width, num_heads]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.is_heads_first:\n",
        "            return inputs + tf.gather(self.pos_bias, self.bias_coords, axis=-1)\n",
        "        else:\n",
        "            return inputs + tf.transpose(tf.gather(self.pos_bias, self.bias_coords, axis=-1), self.transpose_perm)\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        base_config.update({\"input_height\": self.input_height, \"is_heads_first\": self.is_heads_first})\n",
        "        return base_config\n",
        "\n",
        "\n",
        "def LWA(\n",
        "    inputs, kernel_size=7, num_heads=4, key_dim=0, out_weight=True, qkv_bias=True, out_bias=True, attn_dropout=0, output_dropout=0, name=None\n",
        "):\n",
        "    _, hh, ww, cc = inputs.shape\n",
        "    key_dim = key_dim if key_dim > 0 else cc // num_heads\n",
        "    qk_scale = 1.0 / (float(key_dim) ** 0.5)\n",
        "    out_shape = cc\n",
        "    qkv_out = num_heads * key_dim\n",
        "\n",
        "    should_pad_hh, should_pad_ww = max(0, kernel_size - hh), max(0, kernel_size - ww)\n",
        "    if should_pad_hh or should_pad_ww:\n",
        "        inputs = tf.pad(inputs, [[0, 0], [0, should_pad_hh], [0, should_pad_ww], [0, 0]])\n",
        "        _, hh, ww, cc = inputs.shape\n",
        "\n",
        "    qkv = keras.layers.Dense(qkv_out * 3, use_bias=qkv_bias, name=name and name + \"qkv\")(inputs)\n",
        "    query, key_value = tf.split(qkv, [qkv_out, qkv_out * 2], axis=-1)  # Matching weights from PyTorch\n",
        "    query = tf.expand_dims(tf.reshape(query, [-1, hh * ww, num_heads, key_dim]), -2)  # [batch, hh * ww, num_heads, 1, key_dim]\n",
        "\n",
        "    # key_value: [batch, height // kernel_size, width // kernel_size, kernel_size, kernel_size, key + value]\n",
        "    key_value = CompatibleExtractPatches(sizes=kernel_size, strides=1, padding=\"VALID\", compressed=False)(key_value)\n",
        "    padded = (kernel_size - 1) // 2\n",
        "    # torch.pad 'replicate'\n",
        "    key_value = tf.concat([tf.repeat(key_value[:, :1], padded, axis=1), key_value, tf.repeat(key_value[:, -1:], padded, axis=1)], axis=1)\n",
        "    key_value = tf.concat([tf.repeat(key_value[:, :, :1], padded, axis=2), key_value, tf.repeat(key_value[:, :, -1:], padded, axis=2)], axis=2)\n",
        "\n",
        "    key_value = tf.reshape(key_value, [-1, kernel_size * kernel_size, key_value.shape[-1]])\n",
        "    key, value = tf.split(key_value, 2, axis=-1)  # [batch * block_height * block_width, kernel_size * kernel_size, key_dim]\n",
        "    key = tf.transpose(tf.reshape(key, [-1, key.shape[1], num_heads, key_dim]), [0, 2, 3, 1])  # [batch * hh*ww, num_heads, key_dim, kernel_size * kernel_size]\n",
        "    key = tf.reshape(key, [-1, hh * ww, num_heads, key_dim, kernel_size * kernel_size])  # [batch, hh*ww, num_heads, key_dim, kernel_size * kernel_size]\n",
        "    value = tf.transpose(tf.reshape(value, [-1, value.shape[1], num_heads, key_dim]), [0, 2, 1, 3])\n",
        "    value = tf.reshape(value, [-1, hh * ww, num_heads, kernel_size * kernel_size, key_dim])  # [batch, hh*ww, num_heads, kernel_size * kernel_size, key_dim]\n",
        "    # print(f\">>>> {query.shape = }, {key.shape = }, {value.shape = }\")\n",
        "\n",
        "    # [batch, hh * ww, num_heads, 1, kernel_size * kernel_size]\n",
        "    attention_scores = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([query, key]) * qk_scale\n",
        "    attention_scores = MultiHeadRelativePositionalKernelBias(input_height=hh, name=name and name + \"pos\")(attention_scores)\n",
        "    attention_scores = keras.layers.Softmax(axis=-1, name=name and name + \"attention_scores\")(attention_scores)\n",
        "    attention_scores = keras.layers.Dropout(attn_dropout, name=name and name + \"attn_drop\")(attention_scores) if attn_dropout > 0 else attention_scores\n",
        "\n",
        "    # attention_output = [batch, block_height * block_width, num_heads, 1, key_dim]\n",
        "    attention_output = keras.layers.Lambda(lambda xx: tf.matmul(xx[0], xx[1]))([attention_scores, value])\n",
        "    attention_output = tf.reshape(attention_output, [-1, hh, ww, num_heads * key_dim])\n",
        "    # print(f\">>>> {attention_output.shape = }, {attention_scores.shape = }\")\n",
        "\n",
        "    if should_pad_hh or should_pad_ww:\n",
        "        attention_output = attention_output[:, : hh - should_pad_hh, : ww - should_pad_ww, :]\n",
        "\n",
        "    if out_weight:\n",
        "        # [batch, hh, ww, num_heads * key_dim] * [num_heads * key_dim, out] --> [batch, hh, ww, out]\n",
        "        attention_output = keras.layers.Dense(out_shape, use_bias=out_bias, name=name and name + \"output\")(attention_output)\n",
        "    attention_output = keras.layers.Dropout(output_dropout, name=name and name + \"out_drop\")(attention_output) if output_dropout > 0 else attention_output\n",
        "    return attention_output\n",
        "\n",
        "\n",
        "def LWA_block(inputs, attn_kernel_size=7, num_heads=4, mlp_ratio=4, mlp_drop_rate=0, attn_drop_rate=0, drop_rate=0, layer_scale=-1, name=None):\n",
        "    input_channel = inputs.shape[-1]\n",
        "\n",
        "    attn = layer_norm(inputs, name=name + \"attn_\")\n",
        "    attn = LWA(attn, attn_kernel_size, num_heads, attn_dropout=attn_drop_rate, name=name + \"attn_\")\n",
        "    attn = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + \"1_gamma\")(attn) if layer_scale >= 0 else attn\n",
        "    attn = drop_block(attn, drop_rate=drop_rate, name=name + \"attn_\")\n",
        "    attn_out = keras.layers.Add(name=name + \"attn_out\")([inputs, attn])\n",
        "\n",
        "    mlp = layer_norm(attn_out, name=name + \"mlp_\")\n",
        "    mlp = mlp_block(mlp, int(input_channel * mlp_ratio), activation=\"gelu\", name=name + \"mlp_\")\n",
        "    mlp = ChannelAffine(use_bias=False, weight_init_value=layer_scale, name=name + \"2_gamma\")(mlp) if layer_scale >= 0 else mlp\n",
        "    mlp = drop_block(mlp, drop_rate=drop_rate, name=name + \"mlp_\")\n",
        "    return keras.layers.Add(name=name + \"output\")([attn_out, mlp])\n",
        "\n",
        "def FExtractor(inputs):\n",
        "    \n",
        "    x = Conv3D(filters=16, kernel_size=(1, 1, 7), activation='relu', padding='same')(inputs)\n",
        "    x = Conv3D(filters=32, kernel_size=(3, 3, 5), activation='relu',padding='same')(x)\n",
        "    x = Conv3D(filters=64, kernel_size=(5, 5, 7), activation='relu',padding='same')(x)\n",
        "    x_shape = x.shape\n",
        "    x = Reshape((x_shape[1], x_shape[2], x_shape[3]*x_shape[4]))(x)\n",
        "    x = Conv2D(filters=12, kernel_size=(3,3), activation='relu',padding='same')(x)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def PolSARFormer(\n",
        "    num_blocks=[3, 4],\n",
        "    out_channels=[64, 128],\n",
        "    num_heads=[2, 2],\n",
        "    stem_width=-1,\n",
        "    attn_kernel_size=7,\n",
        "    mlp_ratio=3,\n",
        "    layer_scale=-1,\n",
        "    input_shape=(12, 12, 12,1),\n",
        "    num_classes=5,\n",
        "    drop_connect_rate=0,\n",
        "    classifier_activation=\"softmax\",\n",
        "    dropout=0,\n",
        "    pretrained=None,\n",
        "    model_name=\"PolF\",\n",
        "    kwargs=None,\n",
        "):\n",
        "    \"\"\"ConvTokenizer stem\"\"\"\n",
        "    inputs = keras.layers.Input(input_shape)\n",
        "    x=FExtractor(inputs)\n",
        "    \n",
        "    stem_width = stem_width if stem_width > 0 else out_channels[0]\n",
        "    nn = conv2d_no_bias(x, stem_width // 2, kernel_size=3, strides=2, use_bias=True, padding=\"SAME\", name=\"stem_1_\")\n",
        "    nn = conv2d_no_bias(nn, stem_width, kernel_size=3, strides=2, use_bias=True, padding=\"SAME\", name=\"stem_2_\")\n",
        "    nn = layer_norm(nn, name=\"stem_\")\n",
        "\n",
        "    \"\"\" stages \"\"\"\n",
        "    total_blocks = sum(num_blocks)\n",
        "    global_block_id = 0\n",
        "    for stack_id, (num_block, out_channel, num_head) in enumerate(zip(num_blocks, out_channels, num_heads)):\n",
        "        stack_name = \"stack{}_\".format(stack_id + 1)\n",
        "        if stack_id > 0:\n",
        "            ds_name = stack_name + \"downsample_\"\n",
        "            nn = conv2d_no_bias(nn, out_channel, kernel_size=3, strides=2, padding=\"SAME\", name=ds_name)\n",
        "            nn = layer_norm(nn, name=ds_name)\n",
        "        for block_id in range(num_block):\n",
        "            block_name = stack_name + \"block{}_\".format(block_id + 1)\n",
        "            block_drop_rate = drop_connect_rate * global_block_id / total_blocks\n",
        "            nn = LWA_block(nn, attn_kernel_size, num_head, mlp_ratio, drop_rate=block_drop_rate, layer_scale=layer_scale, name=block_name)\n",
        "            global_block_id += 1\n",
        "    nn = layer_norm(nn, name=\"pre_output_\")\n",
        "\n",
        "    nn = output_block(x, num_classes=num_classes, drop_rate=dropout, classifier_activation=classifier_activation)\n",
        "    model = keras.models.Model(inputs, nn, name=model_name)\n",
        "    add_pre_post_process(model, rescale_mode=\"torch\")\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "ycsHZgfqoGc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining your Model .\n",
        "model = PolSARFormer(input_shape=(12, 12, 12,1),out_channels=[64,128],num_heads=[2,2], num_classes=5)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RKacBerQn25R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "dropout_rate = 0.4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "zheiY_sMn28M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####                               K-Fold data validation\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "from sklearn.model_selection import KFold\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return average_acc\n",
        "\n",
        "\n",
        "# Define per-fold score containers\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 5\n",
        "no_epochs = 40\n",
        "optimizer = Adam()\n",
        "verbosity = 1\n",
        "num_folds = 3\n",
        "aa_per_fold = []\n",
        "oa_per_fold = []\n",
        "ki_per_fold = []\n",
        "\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "# Merge inputs and targets\n",
        "inputs = np.concatenate((Xtrain, Xtest), axis=0)\n",
        "targets = np.concatenate((ytrain, ytest), axis=0)\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "fold_no = 1\n",
        "for train, test in kfold.split(inputs, targets):\n",
        "\n",
        "  # Define the model architecture\n",
        "\n",
        "  model = PolSARFormer(input_shape=(12, 12, 12,1),out_channels=[64,128],num_heads=[2,2], num_classes=5)\n",
        "  # Compile the model\n",
        "  # Compile the model\n",
        "  optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "  model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  # Generate a print\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'Training for fold {fold_no} ...')\n",
        "\n",
        "  # Fit data to model\n",
        "  history = model.fit(inputs[train], targets[train],\n",
        "              batch_size=batch_size,\n",
        "              epochs=5)\n",
        "\n",
        "\n",
        "\n",
        "  # Generate generalization metrics\n",
        "  scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
        "  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "    \n",
        "  Y_pred = model.predict(inputs[test])\n",
        "  y_pred = np.argmax(Y_pred, axis=1)\n",
        "  confusion = confusion_matrix(targets[test], y_pred)\n",
        "  oa = accuracy_score(targets[test], y_pred)\n",
        "  \n",
        "  oa_per_fold.append(oa * 100)\n",
        "  aa = AA_andEachClassAccuracy(confusion)\n",
        "  aa_per_fold.append(aa * 100)\n",
        "  kappa = cohen_kappa_score(targets[test], y_pred)\n",
        "  ki_per_fold.append(kappa * 100)\n",
        "\n",
        "\n",
        "  # Increase fold number\n",
        "  fold_no = fold_no + 1\n",
        "    \n",
        "print(f'> OA: {np.mean(oa_per_fold)} (+- {np.std(oa_per_fold)})')\n",
        "print(f'> AA: {np.mean(aa_per_fold)} (+- {np.std(aa_per_fold)})')\n",
        "print(f'> KI: {np.mean(ki_per_fold)} (+- {np.std(ki_per_fold)})')\n"
      ],
      "metadata": {
        "id": "N5nKPyVSn2-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MADN0mGWobIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## Without k-fold data validation\n",
        "\n",
        "optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "checkpoint_filepath = \"E:/San.h5\"\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "history = model.fit(\n",
        "        x=Xtrain,\n",
        "        y=ytrain,\n",
        "        batch_size=batch_size,\n",
        "        epochs=100,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )"
      ],
      "metadata": {
        "id": "LcUWTWt_obLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yi7t9iLUobOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = Xtest.reshape(-1, 12, 12, 12,1)\n",
        "\n",
        "\n",
        "Xtest.shape"
      ],
      "metadata": {
        "id": "TSVKgRxfobRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xKkGS3ZxobUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
        "from operator import truediv\n",
        "\n",
        "def AA_andEachClassAccuracy(confusion_matrix):\n",
        "    counter = confusion_matrix.shape[0]\n",
        "    list_diag = np.diag(confusion_matrix)\n",
        "    list_raw_sum = np.sum(confusion_matrix, axis=1)\n",
        "    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n",
        "    average_acc = np.mean(each_acc)\n",
        "    return average_acc\n",
        "\n",
        "\n",
        "Y_pred = model.predict(Xtest)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "confusion = confusion_matrix(ytest, y_pred)\n",
        "oa = accuracy_score(ytest, y_pred)\n",
        "aa = AA_andEachClassAccuracy(confusion)\n",
        "kappa = cohen_kappa_score(ytest, y_pred)\n",
        "\n",
        "\n",
        "oa, aa, kappa"
      ],
      "metadata": {
        "id": "iD2y5OxjobXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9tuNgIOn3Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kItUt-Tnn3Em"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
